{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'True'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "DATA_DIR = Path(\".\")\n",
    "FILE_CONS  = DATA_DIR/\"20201015_consumption.xlsx\"\n",
    "FILE_PROF  = DATA_DIR/\"20201015_profiles.xlsx\"\n",
    "FILE_WEATH = DATA_DIR/\"20201015_weather.xlsx\"\n",
    "\n",
    "# Keep a subset of meters initially to avoid RAM blow-ups; raise later\n",
    "N_METERS_SAMPLE = 50                  # try 50 first; increase to e.g. 200 later\n",
    "CHECKPOINT_LONG = DATA_DIR/\"long_checkpoint.pkl\"\n",
    "USE_PARQUET_CHECKPOINT = True\n",
    "\n",
    "# Optional: limit threads for native libs to reduce crashes on macOS\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"4\")\n",
    "os.environ.setdefault(\"KMP_DUPLICATE_LIB_OK\", \"TRUE\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: /Users/kaveengarthigeyan/Downloads/spanish-house-energy-consumption\n",
      "LightGBM not importable: No module named 'lightgbm'\n",
      "'model_lgb' exists? True\n",
      "'gmodel' exists? False\n",
      "'hgb' exists? True\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "print(\"CWD:\", os.getcwd())\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    print(\"LightGBM:\", lgb.__version__)\n",
    "except Exception as e:\n",
    "    print(\"LightGBM not importable:\", e)\n",
    "\n",
    "print(\"'model_lgb' exists?\", 'model_lgb' in locals())\n",
    "print(\"'gmodel' exists?\", 'gmodel' in locals())\n",
    "print(\"'hgb' exists?\", 'hgb' in locals())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint: long_checkpoint.pkl (GB=0.037)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26345"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mem_gb(df): \n",
    "    return round(df.memory_usage(deep=True).sum()/1e9, 3)\n",
    "\n",
    "def print_head(df, n=5, title=\"\"):\n",
    "    if title: print(f\"\\n--- {title} ---\")\n",
    "    print(df.head(n).to_string())\n",
    "\n",
    "# ---------- 1) LOAD & RESHAPE ----------\n",
    "if USE_PARQUET_CHECKPOINT and CHECKPOINT_LONG.exists():\n",
    "    long = pd.read_pickle(CHECKPOINT_LONG)\n",
    "    print(f\"Loaded checkpoint: {CHECKPOINT_LONG} (GB={mem_gb(long)})\")\n",
    "else:\n",
    "    print(\"Loading consumption (wide format)…\")\n",
    "    cons = pd.read_excel(FILE_CONS)\n",
    "    cons.columns = cons.columns.map(str).str.strip()\n",
    "    cons = cons.rename(columns={\"date\": \"timestamp\"})\n",
    "    cons[\"timestamp\"] = pd.to_datetime(cons[\"timestamp\"], errors=\"coerce\")\n",
    "    print(\"Consumption shape:\", cons.shape)\n",
    "\n",
    "    # choose a manageable subset of IDs to start\n",
    "    meter_cols = cons.columns.drop(\"timestamp\")\n",
    "    if N_METERS_SAMPLE and len(meter_cols) > N_METERS_SAMPLE:\n",
    "        meter_cols = meter_cols[:N_METERS_SAMPLE]\n",
    "    cons_small = cons[[\"timestamp\"] + meter_cols.tolist()].copy()\n",
    "\n",
    "    # wide -> long\n",
    "    long = cons_small.melt(id_vars=\"timestamp\",\n",
    "                           var_name=\"household_id\",\n",
    "                           value_name=\"consumption_kwh\")\n",
    "    long = long.dropna(subset=[\"timestamp\", \"consumption_kwh\"])\n",
    "    long[\"household_id\"] = long[\"household_id\"].astype(str)\n",
    "    long[\"consumption_kwh\"] = long[\"consumption_kwh\"].astype(\"float32\")\n",
    "    long.sort_values([\"household_id\",\"timestamp\"], inplace=True, ignore_index=True)\n",
    "\n",
    "    print(\"Long shape:\", long.shape, \"GB:\", mem_gb(long))\n",
    "    print_head(long, title=\"Long preview\")\n",
    "\n",
    "    # checkpoint to avoid redoing melt on every restart\n",
    "    if USE_PARQUET_CHECKPOINT:\n",
    "        long.to_pickle(CHECKPOINT_LONG)\n",
    "        print(f\"Checkpoint saved → {CHECKPOINT_LONG}\")\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature engineering…\n",
      "Model_df shape: (429600, 14) GB: 0.055\n",
      "\n",
      "Baseline (lag-168h)…\n",
      "Baseline MAE (lag_168h): 9.846980094909668\n",
      "Baseline MAE (lag_24h / daily seasonal naive): 4.190036773681641\n",
      "ETS baseline skipped: ModuleNotFoundError(\"No module named 'statsmodels'\")\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFeature engineering…\")\n",
    "long.sort_values([\"household_id\",\"timestamp\"], inplace=True, ignore_index=True)\n",
    "\n",
    "# calendar\n",
    "long[\"hour\"] = long[\"timestamp\"].dt.hour.astype(\"int16\")\n",
    "long[\"dow\"] = long[\"timestamp\"].dt.dayofweek.astype(\"int8\")\n",
    "long[\"is_weekend\"] = (long[\"dow\"] >= 5).astype(\"int8\")\n",
    "\n",
    "# cyclic encodings (helpful for daily/annual cycles)\n",
    "long[\"hod_sin\"] = np.sin(2*np.pi*long[\"hour\"]/24).astype(\"float32\")\n",
    "long[\"hod_cos\"] = np.cos(2*np.pi*long[\"hour\"]/24).astype(\"float32\")\n",
    "doy = long[\"timestamp\"].dt.dayofyear\n",
    "long[\"doy_sin\"] = np.sin(2*np.pi*doy/365).astype(\"float32\")\n",
    "long[\"doy_cos\"] = np.cos(2*np.pi*doy/365).astype(\"float32\")\n",
    "\n",
    "# lags/rolling per household\n",
    "grp = long.groupby(\"household_id\")[\"consumption_kwh\"]\n",
    "long[\"lag_1h\"]   = grp.shift(1)\n",
    "long[\"lag_24h\"]  = grp.shift(24)\n",
    "long[\"lag_168h\"] = grp.shift(168)\n",
    "long[\"roll_mean_6h\"] = grp.rolling(6, min_periods=1).mean().values\n",
    "for c in [\"lag_1h\",\"lag_24h\",\"lag_168h\",\"roll_mean_6h\"]:\n",
    "    long[c] = long[c].astype(\"float32\")\n",
    "\n",
    "# usable training rows (need lag_168h)\n",
    "model_df = long.dropna(subset=[\"lag_168h\"]).copy()\n",
    "print(\"Model_df shape:\", model_df.shape, \"GB:\", mem_gb(model_df))\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "print(\"\\nBaseline (lag-168h)…\")\n",
    "model_df[\"baseline_pred\"] = model_df[\"lag_168h\"]\n",
    "mae_baseline = mean_absolute_error(model_df[\"consumption_kwh\"], model_df[\"baseline_pred\"])\n",
    "print(\"Baseline MAE (lag_168h):\", mae_baseline)\n",
    "\n",
    "# 3A) Daily seasonal naive (lag-24h)\n",
    "model_df[\"baseline_day\"] = model_df[\"lag_24h\"]\n",
    "mae_day = mean_absolute_error(model_df[\"consumption_kwh\"], model_df[\"baseline_day\"])\n",
    "print(\"Baseline MAE (lag_24h / daily seasonal naive):\", mae_day)\n",
    "\n",
    "# 3B) ETS baseline (one representative household)\n",
    "try:\n",
    "    from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "    hh = model_df[\"household_id\"].value_counts().index[0]\n",
    "    series = model_df.loc[model_df[\"household_id\"] == hh, \"consumption_kwh\"]\n",
    "    ets = ExponentialSmoothing(series, trend=\"add\", seasonal=\"add\", seasonal_periods=24)\n",
    "    ets_fit = ets.fit()\n",
    "    ets_forecast = ets_fit.forecast(len(series))\n",
    "    mae_ets = mean_absolute_error(series, ets_forecast)\n",
    "    print(\"ETS MAE (single household):\", mae_ets)\n",
    "except Exception as e:\n",
    "    print(\"ETS baseline skipped:\", repr(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model on one household for speed…\n",
      "HGB MAE: 0.010381790744524355 | Δ vs baseline: 9.836598304165143\n",
      "\n",
      "Global LightGBM model (across households)…\n",
      "Global LightGBM skipped: ModuleNotFoundError(\"No module named 'lightgbm'\")\n",
      "LightGBM not run: ModuleNotFoundError(\"No module named 'lightgbm'\")\n"
     ]
    }
   ],
   "source": [
    "# print(\"\\nBaseline (lag-168h)…\")\n",
    "# model_df[\"baseline_pred\"] = model_df[\"lag_168h\"]\n",
    "# mae_baseline = mean_absolute_error(model_df[\"consumption_kwh\"], model_df[\"baseline_pred\"])\n",
    "# print(\"Baseline MAE:\", mae_baseline)\n",
    "\n",
    "# # ---------- 6) TRAIN FAST ON ONE HOUSEHOLD ----------\n",
    "# print(\"\\nTraining model on one household for speed…\")\n",
    "# sample_house = model_df[\"household_id\"].value_counts().index[0]\n",
    "# df_small = model_df.loc[model_df[\"household_id\"] == sample_house].copy()\n",
    "# df_small.sort_values(\"timestamp\", inplace=True)\n",
    "\n",
    "# features = [\n",
    "#     \"hour\",\"dow\",\"is_weekend\",\"hod_sin\",\"hod_cos\",\"doy_sin\",\"doy_cos\",\n",
    "#     \"lag_1h\",\"lag_24h\",\"lag_168h\",\"roll_mean_6h\"\n",
    "# ]\n",
    "# X = df_small[features].astype(\"float32\").fillna(0)\n",
    "# y = df_small[\"consumption_kwh\"].astype(\"float32\")\n",
    "\n",
    "# split = int(len(X)*0.8)\n",
    "# X_train, X_test = X.iloc[:split], X.iloc[split:]\n",
    "# y_train, y_test = y.iloc[:split], y.iloc[split:]\n",
    "\n",
    "# # Stable, fast model (sklearn; no OpenMP headaches)\n",
    "# hgb = HistGradientBoostingRegressor(\n",
    "#     loss=\"absolute_error\", learning_rate=0.05, max_iter=400,\n",
    "#     early_stopping=True, validation_fraction=0.2, random_state=42\n",
    "# )\n",
    "# hgb.fit(X_train, y_train)\n",
    "# preds_hgb = hgb.predict(X_test)\n",
    "# mae_hgb = mean_absolute_error(y_test, preds_hgb)\n",
    "# print(\"HGB MAE:\", mae_hgb, \"| Δ vs baseline:\", mae_baseline - mae_hgb)\n",
    "\n",
    "# # ---------- 7) OPTIONAL: LIGHTGBM (if installed) ----------\n",
    "# try:\n",
    "#     import lightgbm as lgb\n",
    "#     print(\"\\nTrying LightGBM…\")\n",
    "#     train_data = lgb.Dataset(X_train, label=y_train)\n",
    "#     val_data   = lgb.Dataset(X_test,  label=y_test, reference=train_data)\n",
    "#     params = {\n",
    "#         \"objective\": \"regression\",\n",
    "#         \"metric\": \"mae\",\n",
    "#         \"learning_rate\": 0.05,\n",
    "#         \"num_leaves\": 31,\n",
    "#         \"feature_fraction\": 0.9,\n",
    "#         \"bagging_fraction\": 0.9,\n",
    "#         \"bagging_freq\": 5,\n",
    "#         \"verbosity\": -1,\n",
    "#         \"num_threads\": 4,\n",
    "#     }\n",
    "#     model = lgb.train(\n",
    "#         params, train_data, num_boost_round=600, valid_sets=[val_data],\n",
    "#         callbacks=[lgb.early_stopping(stopping_rounds=40), lgb.log_evaluation(100)]\n",
    "#     )\n",
    "#     preds_lgb = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "#     mae_lgb = mean_absolute_error(y_test, preds_lgb)\n",
    "#     print(\"LightGBM MAE:\", mae_lgb, \"| Δ vs baseline:\", mae_baseline - mae_lgb)\n",
    "# except Exception as e:\n",
    "#     print(\"LightGBM not run:\", repr(e))\n",
    "\n",
    "# # ---------- 8) SEGMENTED EVAL (great for slides) ----------\n",
    "# print(\"\\nSegmented evaluation (hour)…\")\n",
    "# test = df_small.iloc[split:].copy()\n",
    "# test[\"pred_hgb\"] = preds_hgb\n",
    "# test[\"baseline\"] = test[\"lag_168h\"]\n",
    "# test[\"err_model\"] = (test[\"consumption_kwh\"]-test[\"pred_hgb\"]).abs()\n",
    "# test[\"err_base\"]  = (test[\"consumption_kwh\"]-test[\"baseline\"]).abs()\n",
    "# seg = test.groupby(\"hour\")[[\"err_model\",\"err_base\"]].mean().rename(\n",
    "#     columns={\"err_model\":\"MAE_model\",\"err_base\":\"MAE_baseline\"}\n",
    "# )\n",
    "# print(seg.head(24).to_string())\n",
    "\n",
    "# print(\"\\nDone.\")\n",
    "\n",
    "\n",
    "print(\"\\nTraining model on one household for speed…\")\n",
    "sample_house = model_df[\"household_id\"].value_counts().index[0]\n",
    "df_small = model_df.loc[model_df[\"household_id\"] == sample_house].copy()\n",
    "df_small.sort_values(\"timestamp\", inplace=True)\n",
    "\n",
    "features = [\n",
    "    \"hour\",\"dow\",\"is_weekend\",\"hod_sin\",\"hod_cos\",\"doy_sin\",\"doy_cos\",\n",
    "    \"lag_1h\",\"lag_24h\",\"lag_168h\",\"roll_mean_6h\"\n",
    "]\n",
    "X = df_small[features].astype(\"float32\").fillna(0)\n",
    "y = df_small[\"consumption_kwh\"].astype(\"float32\")\n",
    "\n",
    "split = int(len(X)*0.8)\n",
    "X_train, X_test = X.iloc[:split], X.iloc[split:]\n",
    "y_train, y_test = y.iloc[:split], y.iloc[split:]\n",
    "\n",
    "hgb = HistGradientBoostingRegressor(\n",
    "    loss=\"absolute_error\", learning_rate=0.05, max_iter=400,\n",
    "    early_stopping=True, validation_fraction=0.2, random_state=42\n",
    ")\n",
    "hgb.fit(X_train, y_train)\n",
    "preds_hgb = hgb.predict(X_test)\n",
    "mae_hgb = mean_absolute_error(y_test, preds_hgb)\n",
    "print(\"HGB MAE:\", mae_hgb, \"| Δ vs baseline:\", mae_baseline - mae_hgb)\n",
    "\n",
    "# ---------- 5) GLOBAL MODEL (multi-household) ----------\n",
    "print(\"\\nGlobal LightGBM model (across households)…\")\n",
    "mae_global = None\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    model_df[\"hh_te\"] = model_df.groupby(\"household_id\")[\"consumption_kwh\"].transform(\"mean\").astype(\"float32\")\n",
    "    features_global = [\"hour\",\"dow\",\"is_weekend\",\"lag_1h\",\"lag_24h\",\"lag_168h\",\"roll_mean_6h\",\"hh_te\"]\n",
    "    Xg = model_df[features_global].fillna(0).astype(np.float32)\n",
    "    yg = model_df[\"consumption_kwh\"].astype(np.float32)\n",
    "    splitg = int(len(Xg)*0.8)\n",
    "    Xg_train, Xg_test = Xg.iloc[:splitg], Xg.iloc[splitg:]\n",
    "    yg_train, yg_test = yg.iloc[:splitg], yg.iloc[splitg:]\n",
    "    gmodel = lgb.LGBMRegressor(objective=\"regression\", learning_rate=0.05, n_estimators=300,\n",
    "                               num_leaves=63, subsample=0.9, subsample_freq=5, colsample_bytree=0.9)\n",
    "    gmodel.fit(Xg_train, yg_train, eval_set=[(Xg_test, yg_test)], eval_metric=\"l1\",\n",
    "               callbacks=[lgb.early_stopping(30), lgb.log_evaluation(50)])\n",
    "    preds_g = gmodel.predict(Xg_test)\n",
    "    mae_global = mean_absolute_error(yg_test, preds_g)\n",
    "    print(\"Global LightGBM MAE:\", mae_global)\n",
    "except Exception as e:\n",
    "    print(\"Global LightGBM skipped:\", repr(e))\n",
    "\n",
    "# ---------- 6) OPTIONAL: LightGBM (single-household point estimate) ----------\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    print(\"\\nSingle-household LightGBM…\")\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    val_data   = lgb.Dataset(X_test,  label=y_test, reference=train_data)\n",
    "    params = {\n",
    "        \"objective\": \"regression\",\n",
    "        \"metric\": \"mae\",\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"num_leaves\": 31,\n",
    "        \"feature_fraction\": 0.9,\n",
    "        \"bagging_fraction\": 0.9,\n",
    "        \"bagging_freq\": 5,\n",
    "        \"verbosity\": -1,\n",
    "        \"num_threads\": 4,\n",
    "    }\n",
    "    model_lgb = lgb.train(\n",
    "        params, train_data, num_boost_round=600, valid_sets=[val_data],\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=40), lgb.log_evaluation(100)]\n",
    "    )\n",
    "    preds_lgb = model_lgb.predict(X_test, num_iteration=model_lgb.best_iteration)\n",
    "    mae_lgb = mean_absolute_error(y_test, preds_lgb)\n",
    "    print(\"LightGBM MAE:\", mae_lgb, \"| Δ vs baseline:\", mae_baseline - mae_lgb)\n",
    "\n",
    "    # ---------- 6A) Quantile LightGBM ----------\n",
    "    print(\"\\nQuantile LightGBM (P10/P50/P90)…\")\n",
    "    preds_q = {}\n",
    "    for q in [0.1, 0.5, 0.9]:\n",
    "        params_q = dict(objective=\"quantile\", alpha=q, metric=\"quantile\", learning_rate=0.05,\n",
    "                        num_leaves=31, feature_fraction=0.9, bagging_fraction=0.9,\n",
    "                        bagging_freq=5, verbosity=-1, num_threads=4)\n",
    "        model_q = lgb.train(params_q, train_data, num_boost_round=400, valid_sets=[val_data],\n",
    "                            callbacks=[lgb.early_stopping(40), lgb.log_evaluation(100)])\n",
    "        preds_q[q] = model_q.predict(X_test, num_iteration=model_q.best_iteration)\n",
    "    pi_width = (preds_q[0.9] - preds_q[0.1]).mean()\n",
    "    coverage = np.mean((y_test >= preds_q[0.1]) & (y_test <= preds_q[0.9])) * 100\n",
    "    print(f\"Avg PI width: {pi_width:.5f} | Coverage: {coverage:.1f}%\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"LightGBM not run:\", repr(e))\n",
    "    model_lgb, preds_lgb = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Segmented evaluation (hour)…\n",
      "      MAE_model  MAE_baseline\n",
      "hour                         \n",
      "0      0.001775      0.000268\n",
      "1      0.003847      0.003958\n",
      "2      0.007451      0.010375\n",
      "3      0.001774      0.000268\n",
      "4      0.001404      0.000268\n",
      "5      0.001359      0.000268\n",
      "6      0.001292      0.000268\n",
      "7      0.000680      0.000254\n",
      "8      0.000103      0.000268\n",
      "9      0.002482      0.002634\n",
      "10     0.018675      0.023056\n",
      "11     0.026350      0.027861\n",
      "12     0.036087      0.076167\n",
      "13     0.022722      0.044708\n",
      "14     0.011916      0.034167\n",
      "15     0.013520      0.036528\n",
      "16     0.017211      0.005806\n",
      "17     0.019458      0.036403\n",
      "18     0.023660      0.056708\n",
      "19     0.012924      0.044972\n",
      "20     0.015552      0.000653\n",
      "21     0.001591      0.000417\n",
      "22     0.002575      0.000375\n",
      "23     0.003663      0.000403\n",
      "\n",
      "Expanding-window backtest (HGB)…\n",
      "Backtest fold MAEs (HGB): [0.07441, 0.040037, 0.010382]\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# imp = pd.Series(model.feature_importance(), index=X.columns).sort_values(ascending=False)\n",
    "# imp.head(10).plot(kind=\"barh\", figsize=(6,4), title=\"Top-10 Feature Importances (LightGBM)\")\n",
    "\n",
    "\n",
    "print(\"\\nSegmented evaluation (hour)…\")\n",
    "test = df_small.iloc[split:].copy()\n",
    "test[\"pred_hgb\"] = preds_hgb\n",
    "test[\"baseline\"] = test[\"lag_168h\"]\n",
    "test[\"err_model\"] = (test[\"consumption_kwh\"]-test[\"pred_hgb\"]).abs()\n",
    "test[\"err_base\"]  = (test[\"consumption_kwh\"]-test[\"baseline\"]).abs()\n",
    "seg = test.groupby(\"hour\")[[\"err_model\",\"err_base\"]].mean().rename(\n",
    "    columns={\"err_model\":\"MAE_model\",\"err_base\":\"MAE_baseline\"}\n",
    ")\n",
    "print(seg.head(24).to_string())\n",
    "\n",
    "print(\"\\nExpanding-window backtest (HGB)…\")\n",
    "folds = [0.6, 0.7, 0.8]\n",
    "fold_maes = []\n",
    "for f in folds:\n",
    "    cut = int(len(X) * f)\n",
    "    m = HistGradientBoostingRegressor(\n",
    "        loss=\"absolute_error\", learning_rate=0.05, max_iter=400,\n",
    "        early_stopping=True, validation_fraction=0.2, random_state=42\n",
    "    )\n",
    "    m.fit(X.iloc[:cut], y.iloc[:cut])\n",
    "    preds_bt = m.predict(X.iloc[cut:])\n",
    "    fold_maes.append(mean_absolute_error(y.iloc[cut:], preds_bt))\n",
    "print(\"Backtest fold MAEs (HGB):\", [round(x, 6) for x in fold_maes])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Residuals & business impact…\n",
      "At 10000 households, daily exposure ≈ €498\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "print(\"\\nResiduals & business impact…\")\n",
    "# Choose best preds available for plots:\n",
    "preds_best = preds_hgb\n",
    "if model_lgb is not None:\n",
    "    preds_best = preds_lgb\n",
    "\n",
    "residuals = y_test - preds_best\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(preds_best, residuals, s=10, alpha=0.5)\n",
    "plt.axhline(0, linestyle=\"--\")\n",
    "plt.title(\"Residuals vs Predicted\")\n",
    "plt.xlabel(\"Predicted\"); plt.ylabel(\"Residuals\")\n",
    "plt.tight_layout(); plt.savefig(\"residuals_vs_predicted.png\"); plt.close()\n",
    "\n",
    "N = 10000  # scale example\n",
    "avg_err_kwh = mean_absolute_error(y_test, preds_best)\n",
    "daily_err_kwh = float(avg_err_kwh) * 24 * N\n",
    "euro_per_kwh = 0.20\n",
    "daily_cost = daily_err_kwh * euro_per_kwh\n",
    "print(f\"At {N} households, daily exposure ≈ €{daily_cost:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recomputed baseline MAE (non-NaN rows): 9.870994567871094\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(10,4))\n",
    "# plt.plot(y_test.values[:200], label=\"Actual\")\n",
    "# plt.plot(preds_hgb[:200], label=\"Predicted\")\n",
    "# plt.legend(); plt.title(\"Predicted vs Actual (HGB, sample 200 points)\")\n",
    "\n",
    "\n",
    "if 'model_lgb' in locals() and model_lgb is not None:\n",
    "    try:\n",
    "        imp = pd.Series(model_lgb.feature_importance(), index=X.columns).sort_values(ascending=False)\n",
    "        ax = imp.head(10).plot(kind=\"barh\", figsize=(6,4), title=\"Top-10 Feature Importances (LightGBM)\")\n",
    "        plt.tight_layout(); plt.savefig(\"feature_importance.png\"); plt.close()\n",
    "    except Exception as e:\n",
    "        print(\"Feature importance plot skipped:\", repr(e))\n",
    "\n",
    "ax = seg.plot(kind=\"bar\", figsize=(8,4), title=\"MAE by Hour (Model vs Baseline)\")\n",
    "plt.tight_layout(); plt.savefig(\"mae_by_hour.png\"); plt.close()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(y_test.values[:200], label=\"Actual\")\n",
    "plt.plot(preds_hgb[:200], label=\"Predicted (HGB)\")\n",
    "plt.legend(); plt.title(\"Predicted vs Actual (HGB, sample 200 points)\")\n",
    "plt.tight_layout(); plt.savefig(\"predicted_vs_actual.png\"); plt.close()\n",
    "\n",
    "# Fair baseline (non-NaN mask) check\n",
    "baseline = model_df[\"consumption_kwh\"].shift(168)\n",
    "mask = ~baseline.isna()\n",
    "mae_base2 = mean_absolute_error(model_df.loc[mask,\"consumption_kwh\"], baseline.loc[mask])\n",
    "print(\"Recomputed baseline MAE (non-NaN rows):\", mae_base2)\n",
    "\n",
    "# ---------- 11) SAVE MODEL ----------\n",
    "import joblib\n",
    "joblib.dump({\"model\": hgb, \"features\": features, \"baseline_mae\": mae_baseline, \"model_mae\": mae_hgb},\n",
    "            \"energy_model.joblib\")\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recomputed baseline MAE: 9.870994567871094\n"
     ]
    }
   ],
   "source": [
    "baseline = model_df[\"consumption_kwh\"].shift(168)\n",
    "mask = ~baseline.isna()\n",
    "mae_base2 = mean_absolute_error(model_df.loc[mask,\"consumption_kwh\"], baseline.loc[mask])\n",
    "print(\"Recomputed baseline MAE:\", mae_base2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['energy_model.joblib']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump({\n",
    "    \"model\": hgb,\n",
    "    \"features\": features,\n",
    "    \"baseline_mae\": mae_baseline,\n",
    "    \"model_mae\": mae_hgb\n",
    "}, \"energy_model.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading consumption…\n",
      "long shape: (700800, 3) GB: 0.06\n",
      "[weather] columns: ['timestamp', '5d6fcd1cf44b0324bc6b7254', '5d6fcd1cf44b0324bc6b7257', '5d6fcd1cf44b0324bc6b725a', '5d6fcd1cf44b0324bc6b725d', '5d6fcd1df44b0324bc6b7260'] ... (total 500)\n",
      "[weather] joining on (timestamp, household_id) with 4,371,240 rows.\n",
      "model_df: (687520, 15) GB: 0.094\n",
      "Baseline MAE (lag_168h): 6.238852024078369\n",
      "HGB MAE: 0.010740 | Δ vs baseline: 6.228112\n",
      "Saved mae_by_hour.png\n",
      "Saved acf_plot.png\n",
      "Saved corr_temp_consumption.png\n",
      "Saved residuals_vs_predicted.png\n",
      "Saved energy_model.joblib\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Spanish household load forecasting — compact, robust pipeline\n",
    "# Files expected (course zip):\n",
    "#   - 20201015_consumption.xlsx  (wide: timestamp + household columns)\n",
    "#   - 20201015_weather.xlsx      (wide: timestamp + *same* household columns OR station IDs)\n",
    "#   - 20201015_profiles.xlsx     (maps household_id to optional weather/station id, varies by zip)\n",
    "# Output:\n",
    "#   - acf_plot.png\n",
    "#   - corr_temp_consumption.png\n",
    "#   - mae_by_hour.png\n",
    "#   - residuals_vs_predicted.png\n",
    "#   - energy_model.joblib\n",
    "# ============================================================\n",
    "import os, gc, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------- CONFIG --------\n",
    "DATA_DIR = Path(\".\")\n",
    "FILE_CONS  = DATA_DIR/\"20201015_consumption.xlsx\"\n",
    "FILE_WEATH = DATA_DIR/\"20201015_weather.xlsx\"\n",
    "FILE_PROF  = DATA_DIR/\"20201015_profiles.xlsx\"\n",
    "\n",
    "# Start with a subset of meters to stay memory friendly; lift later\n",
    "N_METERS_SAMPLE = 80\n",
    "\n",
    "# Optional: limit threads to avoid crashing on some OSes\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"4\")\n",
    "os.environ.setdefault(\"KMP_DUPLICATE_LIB_OK\", \"TRUE\")\n",
    "\n",
    "def mem_gb(df): return round(df.memory_usage(deep=True).sum()/1e9, 3)\n",
    "\n",
    "# -------- 1) LOAD CONSUMPTION (wide → long) --------\n",
    "print(\"Loading consumption…\")\n",
    "cons = pd.read_excel(FILE_CONS)\n",
    "cons.columns = cons.columns.map(str).str.strip()\n",
    "cons = cons.rename(columns={\"date\": \"timestamp\"})\n",
    "cons[\"timestamp\"] = pd.to_datetime(cons[\"timestamp\"], errors=\"coerce\")\n",
    "\n",
    "meter_cols = cons.columns.drop(\"timestamp\")\n",
    "if N_METERS_SAMPLE and len(meter_cols) > N_METERS_SAMPLE:\n",
    "    meter_cols = meter_cols[:N_METERS_SAMPLE]\n",
    "cons_small = cons[[\"timestamp\"] + meter_cols.tolist()].copy()\n",
    "\n",
    "long = cons_small.melt(\n",
    "    id_vars=\"timestamp\",\n",
    "    var_name=\"household_id\",\n",
    "    value_name=\"consumption_kwh\"\n",
    ")\n",
    "long = long.dropna(subset=[\"timestamp\", \"consumption_kwh\"])\n",
    "long[\"household_id\"] = long[\"household_id\"].astype(str)\n",
    "long[\"consumption_kwh\"] = long[\"consumption_kwh\"].astype(\"float32\")\n",
    "long.sort_values([\"household_id\",\"timestamp\"], inplace=True, ignore_index=True)\n",
    "print(\"long shape:\", long.shape, \"GB:\", mem_gb(long))\n",
    "\n",
    "# -------- 2) CALENDAR FEATURES --------\n",
    "long[\"hour\"] = long[\"timestamp\"].dt.hour.astype(\"int16\")\n",
    "long[\"dow\"] = long[\"timestamp\"].dt.dayofweek.astype(\"int8\")\n",
    "long[\"is_weekend\"] = (long[\"dow\"] >= 5).astype(\"int8\")\n",
    "\n",
    "# helpful cyclic encodings\n",
    "long[\"hod_sin\"] = np.sin(2*np.pi*long[\"hour\"]/24).astype(\"float32\")\n",
    "long[\"hod_cos\"] = np.cos(2*np.pi*long[\"hour\"]/24).astype(\"float32\")\n",
    "doy = long[\"timestamp\"].dt.dayofyear\n",
    "long[\"doy_sin\"] = np.sin(2*np.pi*doy/365).astype(\"float32\")\n",
    "long[\"doy_cos\"] = np.cos(2*np.pi*doy/365).astype(\"float32\")\n",
    "\n",
    "# -------- 3) WEATHER: wide → long, join on correct key --------\n",
    "def merge_weather(long_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    w = pd.read_excel(FILE_WEATH)\n",
    "    w.columns = w.columns.map(str).str.strip()\n",
    "    w = w.rename(columns={\"date\": \"timestamp\"})\n",
    "    w[\"timestamp\"] = pd.to_datetime(w[\"timestamp\"], errors=\"coerce\")\n",
    "    print(\"[weather] columns:\", list(w.columns)[:6], \"...\", f\"(total {len(w.columns)})\")\n",
    "\n",
    "    # Are the non-time columns actually household IDs?\n",
    "    non_time_cols = [c for c in w.columns if c != \"timestamp\"]\n",
    "    set_cons_ids = set(long_df[\"household_id\"].unique())\n",
    "    overlap = set(c for c in non_time_cols if c in set_cons_ids)\n",
    "    if overlap:\n",
    "        # Great — the weather wide columns match household_id directly\n",
    "        join_key = \"household_id\"\n",
    "        w_long = w.melt(id_vars=\"timestamp\", var_name=join_key, value_name=\"temperature\")\n",
    "        w_long[join_key] = w_long[join_key].astype(str)\n",
    "        print(f\"[weather] joining on (timestamp, {join_key}) with {len(w_long):,} rows.\")\n",
    "        return long_df.merge(w_long, on=[\"timestamp\", join_key], how=\"left\")\n",
    "\n",
    "    # Else try to map via profiles (find the profiles column that overlaps)\n",
    "    prof = None\n",
    "    try:\n",
    "        prof = pd.read_excel(FILE_PROF)\n",
    "        prof.columns = prof.columns.map(str).str.strip()\n",
    "        # pick the profiles col with best overlap with weather column names\n",
    "        best_key = max(\n",
    "            prof.columns,\n",
    "            key=lambda c: len(set(prof[c].astype(str)) & set(non_time_cols))\n",
    "        )\n",
    "        overlap_cnt = len(set(prof[best_key].astype(str)) & set(non_time_cols))\n",
    "        print(f\"[weather] using profiles key '{best_key}' (overlap {overlap_cnt})\")\n",
    "        prof2 = prof[[best_key]].copy()\n",
    "        prof2[\"household_id\"] = prof2.index.astype(str) if \"household_id\" not in prof.columns else prof[\"household_id\"].astype(str)\n",
    "        prof2[best_key] = prof2[best_key].astype(str)\n",
    "        prof2 = prof2.rename(columns={best_key: \"weather_id\"})\n",
    "        # attach weather_id to long_df\n",
    "        tmp = long_df.merge(prof2[[\"household_id\",\"weather_id\"]], on=\"household_id\", how=\"left\")\n",
    "        w_long = w.melt(id_vars=\"timestamp\", var_name=\"weather_id\", value_name=\"temperature\")\n",
    "        w_long[\"weather_id\"] = w_long[\"weather_id\"].astype(str)\n",
    "        return tmp.merge(w_long, on=[\"timestamp\",\"weather_id\"], how=\"left\").drop(columns=[\"weather_id\"])\n",
    "    except Exception as e:\n",
    "        print(\"[weather] profiles based join failed or not needed:\", e)\n",
    "\n",
    "    # Fallback: pick first numeric non-time column (not ideal, but better than failing)\n",
    "    cand = next((c for c in non_time_cols if pd.api.types.is_numeric_dtype(w[c])), None)\n",
    "    if cand is None:\n",
    "        print(\"[weather] No usable weather column found; skipping temperature.\")\n",
    "        return long_df\n",
    "    print(f\"[weather] fallback: using '{cand}' for all rows (broadcast on timestamp only).\")\n",
    "    w2 = w.rename(columns={cand: \"temperature\"})[[\"timestamp\",\"temperature\"]]\n",
    "    return long_df.merge(w2, on=\"timestamp\", how=\"left\")\n",
    "\n",
    "long = merge_weather(long)\n",
    "\n",
    "# -------- 4) LAGS & ROLLING --------\n",
    "grp = long.groupby(\"household_id\")[\"consumption_kwh\"]\n",
    "long[\"lag_1h\"]   = grp.shift(1)\n",
    "long[\"lag_24h\"]  = grp.shift(24)\n",
    "long[\"lag_168h\"] = grp.shift(168)\n",
    "long[\"roll_mean_6h\"] = grp.rolling(6, min_periods=1).mean().values\n",
    "for c in [\"lag_1h\",\"lag_24h\",\"lag_168h\",\"roll_mean_6h\"]:\n",
    "    long[c] = long[c].astype(\"float32\")\n",
    "\n",
    "# training frame (need lag_168h)\n",
    "model_df = long.dropna(subset=[\"lag_168h\"]).copy()\n",
    "print(\"model_df:\", model_df.shape, \"GB:\", mem_gb(model_df))\n",
    "gc.collect()\n",
    "\n",
    "# -------- 5) BASELINE --------\n",
    "model_df[\"baseline_pred\"] = model_df[\"lag_168h\"]\n",
    "mae_baseline = mean_absolute_error(model_df[\"consumption_kwh\"], model_df[\"baseline_pred\"])\n",
    "print(\"Baseline MAE (lag_168h):\", mae_baseline)\n",
    "\n",
    "# -------- 6) TRAIN FAST MODEL (single household for speed) --------\n",
    "sample_house = model_df[\"household_id\"].value_counts().index[0]\n",
    "df_small = model_df.loc[model_df[\"household_id\"] == sample_house].sort_values(\"timestamp\").copy()\n",
    "\n",
    "features = [\n",
    "    \"hour\",\"dow\",\"is_weekend\",\"hod_sin\",\"hod_cos\",\"doy_sin\",\"doy_cos\",\n",
    "    \"lag_1h\",\"lag_24h\",\"lag_168h\",\"roll_mean_6h\"\n",
    "] + ([\"temperature\"] if \"temperature\" in df_small.columns else [])\n",
    "\n",
    "X = df_small[features].astype(\"float32\").fillna(0)\n",
    "y = df_small[\"consumption_kwh\"].astype(\"float32\")\n",
    "\n",
    "split = int(len(X)*0.8)\n",
    "X_train, X_test = X.iloc[:split], X.iloc[split:]\n",
    "y_train, y_test = y.iloc[:split], y.iloc[split:]\n",
    "\n",
    "hgb = HistGradientBoostingRegressor(\n",
    "    loss=\"absolute_error\", learning_rate=0.05, max_iter=400,\n",
    "    early_stopping=True, validation_fraction=0.2, random_state=42\n",
    ")\n",
    "hgb.fit(X_train, y_train)\n",
    "preds_hgb = hgb.predict(X_test)\n",
    "mae_hgb = mean_absolute_error(y_test, preds_hgb)\n",
    "print(f\"HGB MAE: {mae_hgb:.6f} | Δ vs baseline: {mae_baseline - mae_hgb:.6f}\")\n",
    "\n",
    "# -------- 7) SEGMENTED EVAL (hour) & SAVE BARPLOT --------\n",
    "test = df_small.iloc[split:].copy()\n",
    "test[\"pred_hgb\"] = preds_hgb\n",
    "test[\"baseline\"] = test[\"lag_168h\"]\n",
    "test[\"err_model\"] = (test[\"consumption_kwh\"]-test[\"pred_hgb\"]).abs()\n",
    "test[\"err_base\"]  = (test[\"consumption_kwh\"]-test[\"baseline\"]).abs()\n",
    "seg = test.groupby(\"hour\")[[\"err_model\",\"err_base\"]].mean().rename(\n",
    "    columns={\"err_model\":\"MAE_model\",\"err_base\":\"MAE_baseline\"}\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "seg.plot(kind=\"bar\", ax=plt.gca())\n",
    "plt.title(\"Average Mean Absolute Error by Hour\")\n",
    "plt.xlabel(\"Hour of Day\"); plt.ylabel(\"Mean Absolute Error (kWh)\")\n",
    "plt.legend([\"HGB Model\", \"Baseline (lag_168h)\"])\n",
    "plt.tight_layout(); plt.savefig(\"mae_by_hour.png\", dpi=300); plt.close()\n",
    "print(\"Saved mae_by_hour.png\")\n",
    "\n",
    "# -------- 8) AUTOCORRELATION (NumPy, no statsmodels) & SAVE --------\n",
    "series = df_small[\"consumption_kwh\"].astype(float).to_numpy()\n",
    "series = series[np.isfinite(series)]\n",
    "series = series - series.mean()\n",
    "max_lag = 200\n",
    "acf_full = np.correlate(series, series, mode='full')\n",
    "acf = acf_full[acf_full.size//2:][:max_lag+1] / acf_full[acf_full.size//2]\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.bar(np.arange(len(acf)), acf, width=0.9)\n",
    "plt.title(\"Autocorrelation of Hourly Electricity Consumption\")\n",
    "plt.xlabel(\"Lag (hours)\"); plt.ylabel(\"Autocorrelation\")\n",
    "plt.tight_layout(); plt.savefig(\"acf_plot.png\", dpi=300); plt.close()\n",
    "print(\"Saved acf_plot.png\")\n",
    "\n",
    "# -------- 9) TEMP vs CONSUMPTION (hexbin) & SAVE --------\n",
    "if \"temperature\" in model_df.columns:\n",
    "    dfp = model_df[[\"temperature\",\"consumption_kwh\"]].dropna()\n",
    "    # drop obviously nonsense values for a readable plot (does not affect training)\n",
    "    dfp = dfp[dfp[\"consumption_kwh\"].between(0, 25)]\n",
    "    plt.figure(figsize=(8,6))\n",
    "    hb = plt.hexbin(dfp[\"temperature\"], dfp[\"consumption_kwh\"], gridsize=60, mincnt=5, cmap=\"Blues\")\n",
    "    plt.colorbar(hb, label=\"count\")\n",
    "    plt.title(\"Temperature vs Electricity Consumption (household-hour)\")\n",
    "    plt.xlabel(\"Temperature (°C)\"); plt.ylabel(\"Consumption (kWh)\")\n",
    "    plt.tight_layout(); plt.savefig(\"corr_temp_consumption.png\", dpi=300); plt.close()\n",
    "    print(\"Saved corr_temp_consumption.png\")\n",
    "else:\n",
    "    print(\"Skipped corr_temp_consumption.png (no temperature column after merge).\")\n",
    "\n",
    "# -------- 10) RESIDUALS vs PREDICTED (scatter) & SAVE --------\n",
    "residuals = y_test - preds_hgb\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.scatter(preds_hgb, residuals, s=12, alpha=0.45)\n",
    "plt.axhline(0, color=\"black\", linestyle=\"--\")\n",
    "plt.title(\"Residuals vs Predicted Consumption\")\n",
    "plt.xlabel(\"Predicted Consumption (kWh)\"); plt.ylabel(\"Residuals (kWh)\")\n",
    "plt.tight_layout(); plt.savefig(\"residuals_vs_predicted.png\", dpi=300); plt.close()\n",
    "print(\"Saved residuals_vs_predicted.png\")\n",
    "\n",
    "# -------- 11) SAVE MODEL SNIPPET --------\n",
    "import joblib\n",
    "joblib.dump({\n",
    "    \"model\": hgb,\n",
    "    \"features\": features,\n",
    "    \"baseline_mae\": float(mae_baseline),\n",
    "    \"model_mae\": float(mae_hgb),\n",
    "    \"household_id\": sample_house\n",
    "}, \"energy_model.joblib\")\n",
    "print(\"Saved energy_model.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marange(\u001b[39m0\u001b[39m, max_lag\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m), np\u001b[39m.\u001b[39marray(ac)\n\u001b[1;32m     24\u001b[0m \u001b[39m# Use one long household (or aggregate) for a clean ACF\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m ts \u001b[39m=\u001b[39m (df\u001b[39m.\u001b[39msort_values([\u001b[39m\"\u001b[39m\u001b[39mhousehold_id\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mtimestamp\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     26\u001b[0m         \u001b[39m.\u001b[39mgroupby(\u001b[39m\"\u001b[39m\u001b[39mtimestamp\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m\"\u001b[39m\u001b[39mconsumption_kwh\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mmean())  \u001b[39m# city/overall average\u001b[39;00m\n\u001b[1;32m     27\u001b[0m lags, ac_vals \u001b[39m=\u001b[39m acf(ts, max_lag\u001b[39m=\u001b[39m\u001b[39m200\u001b[39m)\n\u001b[1;32m     29\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m12\u001b[39m,\u001b[39m4\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==== 0. Setup ====\n",
    "import os, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.makedirs(\"Images\", exist_ok=True)\n",
    "\n",
    "# Helper to nice-save\n",
    "def savefig(path):\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# ==== 1) ACF without statsmodels (fast, no extra deps) ====\n",
    "def acf(series, max_lag=200):\n",
    "    x = series.values.astype(float)\n",
    "    x = x - x.mean()\n",
    "    denom = (x**2).sum()\n",
    "    ac = [1.0]\n",
    "    for k in range(1, max_lag+1):\n",
    "        num = (x[:-k] * x[k:]).sum()\n",
    "        ac.append(num/denom)\n",
    "    return np.arange(0, max_lag+1), np.array(ac)\n",
    "\n",
    "# Use one long household (or aggregate) for a clean ACF\n",
    "ts = (df.sort_values([\"household_id\",\"timestamp\"])\n",
    "        .groupby(\"timestamp\")[\"consumption_kwh\"].mean())  # city/overall average\n",
    "lags, ac_vals = acf(ts, max_lag=200)\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.bar(lags[1:], ac_vals[1:], width=0.9)\n",
    "plt.title(\"Autocorrelation of Hourly Electricity Consumption\")\n",
    "plt.xlabel(\"Lag (hours)\"); plt.ylabel(\"Autocorrelation\")\n",
    "savefig(\"Images/acf_plot.png\")\n",
    "\n",
    "# ==== 2) Temperature vs consumption (hexbin for dense data) ====\n",
    "plt.figure(figsize=(8,6))\n",
    "hb = plt.hexbin(df[\"temperature\"], df[\"consumption_kwh\"], gridsize=60, mincnt=1)\n",
    "plt.colorbar(hb, label=\"count\")\n",
    "plt.title(\"Temperature vs Electricity Consumption (household-hour)\")\n",
    "plt.xlabel(\"Temperature (°C)\"); plt.ylabel(\"Consumption (kWh)\")\n",
    "savefig(\"Images/corr_temp_consumption.png\")\n",
    "\n",
    "# ==== 3) Residuals vs predicted ====\n",
    "pred_df[\"resid\"] = pred_df[\"y_true\"] - pred_df[\"y_pred\"]\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(pred_df[\"y_pred\"], pred_df[\"resid\"], s=14, alpha=0.35)\n",
    "plt.axhline(0, ls=\"--\", c=\"k\")\n",
    "plt.title(\"Residuals vs Predicted Consumption\")\n",
    "plt.xlabel(\"Predicted Consumption (kWh)\"); plt.ylabel(\"Residuals (kWh)\")\n",
    "savefig(\"Images/residuals_vs_predicted.png\")\n",
    "\n",
    "# ==== 4) MAE by hour: model vs baseline ====\n",
    "mae_hour = (pred_df.assign(\n",
    "    err_hgb=(pred_df[\"y_true\"]-pred_df[\"y_pred\"]).abs(),\n",
    "    err_base=(pred_df[\"y_true\"]-pred_df[\"y_base\"]).abs()\n",
    ").groupby(\"hour\")[[\"err_hgb\",\"err_base\"]].mean())\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.bar(mae_hour.index-0.2, mae_hour[\"err_hgb\"], width=0.4, label=\"HGB Model\")\n",
    "plt.bar(mae_hour.index+0.2, mae_hour[\"err_base\"], width=0.4, label=\"Baseline (lag_168h)\")\n",
    "plt.xticks(range(24)); plt.legend()\n",
    "plt.title(\"Average Mean Absolute Error by Hour\")\n",
    "plt.xlabel(\"Hour of Day\"); plt.ylabel(\"Mean Absolute Error (kWh)\")\n",
    "savefig(\"Images/mae_by_hour.png\")\n",
    "\n",
    "# ==== 5) Predicted vs actual for one sample series (pretty line plot) ====\n",
    "sample_ids = pred_df[\"household_id\"].dropna().unique()\n",
    "hh = sample_ids[0]\n",
    "ex = pred_df[pred_df[\"household_id\"]==hh].sort_values(\"timestamp\").tail(24*4)   # last 4 days\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(ex[\"timestamp\"], ex[\"y_true\"], label=\"Actual\", lw=2)\n",
    "plt.plot(ex[\"timestamp\"], ex[\"y_pred\"], label=\"HGB\", lw=2, alpha=0.9)\n",
    "plt.plot(ex[\"timestamp\"], ex[\"y_base\"], label=\"Baseline\", lw=1.5, alpha=0.7)\n",
    "plt.title(f\"Predicted vs Actual (household {hh})\")\n",
    "plt.xlabel(\"Time\"); plt.ylabel(\"Consumption (kWh)\"); plt.legend()\n",
    "savefig(\"Images/predicted_vs_actual.png\")\n",
    "\n",
    "# ==== 6) Feature importance (HGB supports it) ====\n",
    "if hasattr(hgb, \"feature_importances_\"):\n",
    "    fi = pd.Series(hgb.feature_importances_, index=hgb.feature_names_in_).sort_values(ascending=True).tail(20)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.barh(fi.index, fi.values)\n",
    "    plt.title(\"Feature Importance (HGB)\")\n",
    "    plt.xlabel(\"Importance\")\n",
    "    savefig(\"Images/feature_importance.png\")\n",
    "\n",
    "# ==== 7) Baseline vs Model error comparison across households ====\n",
    "err_by_hh = (pred_df.assign(\n",
    "    mae_hgb=(pred_df[\"y_true\"]-pred_df[\"y_pred\"]).abs(),\n",
    "    mae_base=(pred_df[\"y_true\"]-pred_df[\"y_base\"]).abs()\n",
    ").groupby(\"household_id\")[[\"mae_hgb\",\"mae_base\"]].mean().sort_values(\"mae_base\"))\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(err_by_hh.index, err_by_hh[\"mae_base\"], label=\"Baseline\", lw=1.5)\n",
    "plt.plot(err_by_hh.index, err_by_hh[\"mae_hgb\"], label=\"HGB\", lw=1.5)\n",
    "plt.xticks([], [])  # many households: hide tick clutter\n",
    "plt.title(\"Error comparison across households (MAE)\")\n",
    "plt.ylabel(\"MAE (kWh)\"); plt.legend()\n",
    "savefig(\"Images/error_comparison.png\")\n",
    "\n",
    "# ==== 8) Business Value visuals ====\n",
    "# Configure these to your case:\n",
    "C = 0.10   # $ per kWh absolute error (example)\n",
    "MWh = 1.0  # average hourly load per household (MWh) or total portfolio MWh per hour\n",
    "OH = pred_df[\"timestamp\"].nunique()  # number of forecast hours in eval set\n",
    "\n",
    "# Costs based on MAE (model vs baseline)\n",
    "mae_model = (pred_df[\"y_true\"]-pred_df[\"y_pred\"]).abs().mean()\n",
    "mae_base  = (pred_df[\"y_true\"]-pred_df[\"y_base\"]).abs().mean()\n",
    "\n",
    "cost_model = mae_model * C * MWh * OH\n",
    "cost_base  = mae_base  * C * MWh * OH\n",
    "savings    = cost_base - cost_model\n",
    "\n",
    "# 8a) Cost vs. model chart\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.bar([\"Baseline cost\",\"Model cost\"], [cost_base, cost_model], color=[\"#d37\",\"#37d\"])\n",
    "plt.title(\"Error-Cost Comparison\"); plt.ylabel(\"Total cost ($)\")\n",
    "for i,v in enumerate([cost_base, cost_model]):\n",
    "    plt.text(i, v, f\"${v:,.0f}\", ha=\"center\", va=\"bottom\")\n",
    "savefig(\"Images/business_cost_comparison.png\")\n",
    "\n",
    "# 8b) Cumulative savings over time\n",
    "pred_df = pred_df.sort_values(\"timestamp\")\n",
    "inst_base  = (pred_df[\"y_true\"]-pred_df[\"y_base\"]).abs()  * C * MWh\n",
    "inst_model = (pred_df[\"y_true\"]-pred_df[\"y_pred\"]).abs() * C * MWh\n",
    "cum_sav = (inst_base - inst_model).groupby(pred_df[\"timestamp\"]).mean().cumsum()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(cum_sav.index, cum_sav.values)\n",
    "plt.title(\"Cumulative Savings from Model vs Baseline\")\n",
    "plt.xlabel(\"Time\"); plt.ylabel(\"Cumulative $\")\n",
    "savefig(\"Images/business_cumulative_savings.png\")\n",
    "\n",
    "# 8c) Sensitivity to price and volume (tornado-style simple)\n",
    "scen = []\n",
    "for c in [C*0.5, C, C*1.5]:\n",
    "    scen.append((c, MWh, (mae_base-mae_model)*c*MWh*OH))\n",
    "for m in [MWh*0.7, MWh, MWh*1.3]:\n",
    "    scen.append((C, m, (mae_base-mae_model)*C*m*OH))\n",
    "sens = pd.DataFrame(scen, columns=[\"C\",\"MWh\",\"Savings\"])\n",
    "labels = [\"C -50%\",\"C\",\"C +50%\",\"MWh -30%\",\"MWh\",\"MWh +30%\"]\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.barh(labels, sens[\"Savings\"])\n",
    "plt.title(\"Sensitivity of Savings to Price (C) and Volume (MWh)\")\n",
    "plt.xlabel(\"Savings ($)\")\n",
    "savefig(\"Images/business_sensitivity.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CS421)",
   "language": "python",
   "name": "cs421_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
